#[version = "0.0.5"]
def @main(%conv2d_1_input: Tensor[(1, 1, 28, 28), float32], %v_param_1: Tensor[(6, 1, 3, 3), float32], %v_param_2: Tensor[(6), float32], %v_param_3: Tensor[(16, 6, 3, 3), float32], %v_param_4: Tensor[(16), float32], %v_param_5: Tensor[(84, 784), float32], %v_param_6: Tensor[(84), float32], %v_param_7: Tensor[(10, 84), float32], %v_param_8: Tensor[(10), float32]) {
  %0 = nn.conv2d(%conv2d_1_input, %v_param_1, padding=[1i64, 1i64, 1i64, 1i64], channels=6, kernel_size=[3, 3]);
  %1 = nn.bias_add(%0, %v_param_2);
  %2 = nn.relu(%1);
  %3 = nn.avg_pool2d(%2, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);
  %4 = nn.conv2d(%3, %v_param_3, padding=[1i64, 1i64, 1i64, 1i64], channels=16, kernel_size=[3, 3]);
  %5 = nn.bias_add(%4, %v_param_4);
  %6 = nn.relu(%5);
  %7 = nn.avg_pool2d(%6, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);
  %8 = transpose(%7, axes=[0, 2, 3, 1]);
  %9 = nn.batch_flatten(%8);
  %10 = nn.dense(%9, %v_param_5, units=84);
  %11 = nn.bias_add(%10, %v_param_6);
  %12 = nn.relu(%11);
  %13 = nn.dense(%12, %v_param_7, units=10);
  %14 = nn.bias_add(%13, %v_param_8);
  nn.softmax(%14, axis=1)
}
